---
title: "Legionella delay"
author: "Dan Weinberger"
date: '2022-08-03'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(dplyr)
library(reshape2)
library(rjags)
library(stringr)
library(gtools)

library(HDInterval)
```

## Background
The goal for this analysis is to reconstruct a time series of legionella infections using a time series based on symptom onset date and a delay distribution. This latent time series will then be linked to climate variables. This will be accomplished in a Bayesian framework using rjags.



### First, generate a time series based on exposure date..we will then apply a delay distribution to this to get an 'observed' time series, and see if we can then

```{r}
set.seed(123)
df1  <- cbind.data.frame('dates'=seq.Date(from=as.Date('2012-01-01'), to=as.Date('2022-07-01'), by='day'))

df1 <- df1 %>%
  mutate(t=row_number(), 
         sin1= 2*sin(2*pi*t/365),
         N_exposures = rpois(length(dates), exp(1 + 0.3*sin1 + rnorm(length(dates),0,0.75))) ) 

plot(df1$N_exposures)
```

Distribution to estimate the time between exposure and symptom onset. It's bounded at 20 days which is about the uppermost end.
```{r}
x <- seq(0, 20, by=1)
delay_dist = dgamma(x, shape = 4.96, scale = 1.27, log = FALSE)
plot(delay_dist, type = 'l')

#rmultinom(1,100,delay_dist)
```
Take random draws from a multinomial, based on N exosures on that day and the delay distribution to determine how many cases that were exposed on date t have symptom onset with a delay of d, with d{1:21}

```{r}
set.seed(1234)
dist_data <- t(sapply(df1$N_exposures, function(x) rmultinom(1,x,delay_dist)))

df2 <- cbind.data.frame('dates'=df1$dates, dist_data)

# Reshape the date into long format, determine the symptom onset date based on exposure date + delay, then sum by symptom onset date to get 'observed' time series

observed_ts <- melt(df2, id.vars='dates') %>%
  rename(exposure_date=dates) %>%
  mutate(delay = as.numeric(as.character(variable)),
         symptom_date= exposure_date + delay) %>%
  group_by(symptom_date) %>%
  summarize(N_cases=sum(value))
```

Compare the time series of exposure and time series of cases
```{r, fig.width=6, fig.height=3}
par(mfrow=c(1,2))

plot(df1$N_exposures, type='l', main='N exposures', bty='l')
  
plot(observed_ts$N_cases, type='l', main="N cases based on sympom onset", bty='l')

```

OK, now we can try to go backwards, let's reconstruct the exposure time series based on the case time series. related to deconvolution approach of Lipsitch https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2796142/

- lambda is the mean of the observed cases, based on symptom onset
- Delta is the reporting triangle that links observed time series (lambda) and latent time series (epsilon)

- epsilon follows an AR(1), which captures autocorrelation in the data; This is a base model with no explanatory variables. Epsilon could be a function of weather variables, etc 


Intuition for dirichlet prior. Looks like 'scale10' might be a reasonable level of information
```{r}
delay_dist_scale1 <- delay_dist/max(delay_dist)*1
delay_dist_scale10 <- delay_dist/max(delay_dist)*10
delay_dist_scale100 <- delay_dist/max(delay_dist)*100
delay_dist_scale1000 <- delay_dist/max(delay_dist)*1000

test1 <- t(rdirichlet(100, delay_dist_scale1))
test10 <- t(rdirichlet(100, delay_dist_scale10))
test100 <- t(rdirichlet(100, delay_dist_scale100))
test1000 <- t(rdirichlet(100, delay_dist_scale1000))

par(mfrow=c(2,2))
matplot(test1, type='l')
matplot(test10, type='l')
matplot(test100, type='l')
matplot(test1000, type='l')

```



```{r}
model_string <- "
model{
for(t in 22:N_times){
  N_cases[t] ~ dpois(lambda[t])
  
  lambda[t] <- sum(delta[t,]) 
}

for(t in 1:N_times){
for(d in (1:21)){ #triangle incomplete for first 21 time points
  delta[(t+d-1),d] <-  epsilon[t]*delay_est[d] #the t+d-1 is the key here, adds the delay, subtract 1 bc smallest delay is 0
   } 

epsilon[t] <- exp(int + alpha[t])
 }

  int ~ dnorm(0, 1e-4)
 alpha[1] ~ dnorm(0, tau2.alpha/(1-rho^2))

 for( t in 2:N_times ){
  alpha[t] ~ dnorm(rho*alpha[t-1],tau2.alpha)
 }

    tau2.alpha ~ dgamma(0.01,0.01)
    
    rho ~ dunif (-1,1)
    
   delay_est ~ ddirch(delay_dist_scale10) #use values from literature as priors for the dirichlet

}
"



##############################################################
#Model Fitting
##############################################################
inits1=list(".RNG.seed"=c(123), ".RNG.name"='base::Wichmann-Hill')
inits2=list(".RNG.seed"=c(456), ".RNG.name"='base::Wichmann-Hill')
inits3=list(".RNG.seed"=c(789), ".RNG.name"='base::Wichmann-Hill')


##############################################
#Model Organization
##############################################
model_spec<-textConnection(model_string)
model_jags<-jags.model(model_spec, 
                       inits=list(inits1,inits2, inits3),
                       data=list('N_cases'=observed_ts$N_cases,
                                 'N_times'=365, #could use whole dataset but would take a long time
                                                       'delay_dist_scale10'=delay_dist_scale10
                                    ),
                       n.adapt=10000, 
                       n.chains=3)


params<-c('lambda', 'alpha', 'delay_est','epsilon')

##############################################
#Posterior Sampling
##############################################
posterior_samples<-coda.samples(model_jags, 
                                params, 
                                n.iter=10000)

posterior_samples.all<-do.call(rbind,posterior_samples)
#post1.summary<-summary(posterior_samples)
#post_means<-colMeans(posterior_samples.all)
post_means<-apply(posterior_samples.all, 2, mean)
sample.labs<-names(post_means)
ci<-t(hdi(posterior_samples.all, credMass = 0.95))
#ci<-matrix(sprintf("%.1f",round(ci,1)), ncol=2)
ci<-matrix(ci, ncol=2)

row.names(ci)<-sample.labs
#post_means<-sprintf("%.1f",round(post_means,1))
names(post_means)<-sample.labs

index_n <- as.numeric(unlist(str_extract_all(names(post_means), "(?<=\\[).+?(?=\\])")))

combined <- cbind.data.frame(post_means, ci,index_n )
names(combined) <- c('mean','lcl','ucl','index_t')

post_epsilon <- combined[grep('epsilon',names(post_means)),]

post_lambda <- combined[grep('lambda',names(post_means)),]

#model vs observed TS
plot(observed_ts$N_cases[1:365], type='l')
points(post_epsilon$index_t, post_epsilon$mean, type='l' ,col='red')
points(post_lambda$index_t,post_lambda$mean, type='l' ,col='blue')

#model vs true exposure TS

plot(df1$N_exposures[1:365], type='l')
points(post_epsilon$index_t, post_epsilon$mean, type='l' ,col='red')
points(post_lambda$index_t,post_lambda$mean, type='l' ,col='blue')

str(posterior_samples.all)
```

```{r}
post_delay_est <- combined[grep('delay_est',names(post_means)),]

plot(post_delay_est$mean, type='l', ylim=c(0,1))
points(delay_dist, type='l', lty=2, lwd=2, col='red')


post_samples_delay <- posterior_samples.all[,grep('delay_est',names(post_means))]

apply(post_samples_delay[1:100,],1,sum)

matplot(t(post_samples_delay[1:2000,]), type='l')
points(delay_dist, col='red', lwd=2, lty=2, type='l')

```



